{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3efd91-8f7d-40a6-aef5-3ea1c4a5dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependence \n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b6768b-c191-4ada-b45c-173231a7ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "descs = [desc_name[0] for desc_name in Descriptors._descList]\n",
    "desc_calc = MoleculeDescriptors.MolecularDescriptorCalculator(descs)\n",
    "\n",
    "df = pd.read_csv('./RuCHFunctionalizationDataset/dataset.csv')\n",
    "df_DG = pd.read_csv('./RuCHFunctionalizationDataset/DG.csv')\n",
    "df_RX = pd.read_csv('./RuCHFunctionalizationDataset/RX.csv')\n",
    "\n",
    "num = df['number'].to_list()\n",
    "num = np.array(num)\n",
    "target = df['tag'].to_numpy()\n",
    "\n",
    "target_DG_columns = ['lumo', 'EA', 'bv-3.5']\n",
    "target_DG = df_DG[target_DG_columns].to_numpy()\n",
    "\n",
    "target_RX_columns = ['somo', 'bv-3.5','spin']\n",
    "target_RX = df_RX[target_RX_columns].to_numpy()\n",
    "\n",
    "DG_smiles = df['DG'].to_list()\n",
    "RX_smiles = df['RX'].to_list()\n",
    "cat_smiles = df['catalyst'].to_list()\n",
    "sol_smiles = df['solvent'].to_list()\n",
    "lig_smiles = df['ligand'].to_list()\n",
    "ad_smiles = df['addictive'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41a5fb9-e194-48e9-8062-e02dbe91906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate discriptors\n",
    "def gen_mol_form_smi(smi_list):\n",
    "    mol_list = []\n",
    "    for smi in smi_list:\n",
    "        if isinstance(smi, str) == True :\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol == None:\n",
    "                print(smi)\n",
    "            mol_list.append(mol)\n",
    "        else:\n",
    "            mol = 0\n",
    "            mol_list.append(mol)\n",
    "    return mol_list\n",
    "\n",
    "DG_mols =  gen_mol_form_smi(DG_smiles)\n",
    "RX_mols =  gen_mol_form_smi(RX_smiles)\n",
    "cat_mols =  gen_mol_form_smi(cat_smiles)\n",
    "sol_mols =  gen_mol_form_smi(sol_smiles)\n",
    "lig_mols =  gen_mol_form_smi(lig_smiles)\n",
    "ad_mols = gen_mol_form_smi(ad_smiles)\n",
    "        \n",
    "def gen_desc_from_mol(mol_list):\n",
    "    desc_list = []\n",
    "    for mol in mol_list:\n",
    "        if mol != 0:\n",
    "            desc = desc_calc.CalcDescriptors(mol)\n",
    "            desc_list.append(desc)\n",
    "        else:\n",
    "            n = 208\n",
    "            decs = [0]*n\n",
    "            desc_list.append(decs)\n",
    "    decs = np.array(desc_list)\n",
    "    return decs\n",
    "\n",
    "DG_descs = gen_desc_from_mol(DG_mols)\n",
    "RX_descs = gen_desc_from_mol(RX_mols)\n",
    "cat_descs = gen_desc_from_mol(cat_mols)\n",
    "sol_descs = gen_desc_from_mol(sol_mols)\n",
    "lig_descs = gen_desc_from_mol(lig_mols)\n",
    "ad_descs = gen_desc_from_mol(ad_mols)\n",
    "\n",
    "f_p_2 = df_DG['f+2'].to_numpy()\n",
    "f_p_3 = df_DG['f+3'].to_numpy()\n",
    "f_p_4 = df_DG['f+4'].to_numpy()\n",
    "f_p_5 = df_DG['f+5'].to_numpy()\n",
    "f_p_6 = df_DG['f+6'].to_numpy()\n",
    "\n",
    "f_m_2 = df_DG['f-2'].to_numpy()\n",
    "f_m_3 = df_DG['f-3'].to_numpy()\n",
    "f_m_4 = df_DG['f-4'].to_numpy()\n",
    "f_m_5 = df_DG['f-5'].to_numpy()\n",
    "f_m_6 = df_DG['f-6'].to_numpy()\n",
    "\n",
    "f_0_2 = df_DG['f0-2'].to_numpy()\n",
    "f_0_3 = df_DG['f0-3'].to_numpy()\n",
    "f_0_4 = df_DG['f0-4'].to_numpy()\n",
    "f_0_5 = df_DG['f0-5'].to_numpy()\n",
    "f_0_6 = df_DG['f0-6'].to_numpy()\n",
    "\n",
    "Q_2 = df_DG['Q2'].to_numpy()\n",
    "Q_3 = df_DG['Q3'].to_numpy()\n",
    "Q_4 = df_DG['Q4'].to_numpy()\n",
    "Q_5 = df_DG['Q5'].to_numpy()\n",
    "Q_6 = df_DG['Q6'].to_numpy()\n",
    "\n",
    "DG_phys = np.column_stack([f_p_2, f_p_3, f_p_4, f_p_5, f_p_6, f_m_2, f_m_3, f_m_4, f_m_5, f_m_6,\n",
    "                          f_0_2, f_0_3, f_0_4, f_0_5, f_0_6, Q_2, Q_3, Q_4, Q_5, Q_6])\n",
    "\n",
    "Qc_R = df_RX['Qc'].to_numpy()\n",
    "f0 = df_RX['f0'].to_numpy()\n",
    "f_m = df_RX['f-'].to_numpy()\n",
    "f_p = df_RX['f+'].to_numpy()\n",
    "\n",
    "RX_phys = np.column_stack([Qc_R, f0, f_m, f_p])\n",
    "\n",
    "#feature clean\n",
    "all_descriptors = np.concatenate([DG_descs, RX_descs, cat_descs, lig_descs, sol_descs, ad_descs, DG_phys, RX_phys],axis=1)\n",
    "all_descriptors = pd.DataFrame(all_descriptors)\n",
    "all_descriptors = all_descriptors.dropna(axis=1,how='any')\n",
    "all_descriptors= np.array(all_descriptors)\n",
    "all_descriptors = np.unique(all_descriptors, axis=1)\n",
    "\n",
    "DG_descs = pd.DataFrame(DG_descs)\n",
    "DG_descs = DG_descs.dropna(axis=1,how='any')\n",
    "DG_descs = np.array(DG_descs)\n",
    "DG_descs = np.unique(DG_descs, axis=1)\n",
    "\n",
    "RX_descs = pd.DataFrame(RX_descs)\n",
    "RX_descs = RX_descs.dropna(axis=1,how='any')\n",
    "RX_descs = np.array(RX_descs)\n",
    "RX_descs = np.unique(RX_descs, axis=1)\n",
    "\n",
    "reaction_dim = len(all_descriptors[0])\n",
    "DG_dim = len(DG_descs[0])\n",
    "RX_dim = len(RX_descs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737f360b-f879-447f-a974-d504f70fe344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find substituted arenes\n",
    "def ortho_substituted(mol):\n",
    "    mol_no_H = AllChem.RemoveHs(mol)\n",
    "    for idx, atom in enumerate(mol_no_H.GetAtoms()):\n",
    "        if idx == 1 :\n",
    "            if atom.GetDegree() > 2:\n",
    "                return True\n",
    "        elif idx == 5:\n",
    "            if atom.GetDegree() > 2:\n",
    "                return True\n",
    "            \n",
    "def meta_substituted(mol):\n",
    "    mol_no_H = AllChem.RemoveHs(mol)\n",
    "    for idx, atom in enumerate(mol_no_H.GetAtoms()):\n",
    "        if idx == 2 :\n",
    "            if atom.GetDegree() > 2:\n",
    "                return True\n",
    "        elif idx == 4:\n",
    "            if atom.GetDegree() > 2:\n",
    "                return True\n",
    "            \n",
    "\n",
    "ortho_sub_list = []\n",
    "meta_sub_list = []\n",
    "\n",
    "for idx, mol in enumerate(DG_mols):\n",
    "    if ortho_substituted(mol) == True:\n",
    "        ortho_sub_list.append(idx)\n",
    "\n",
    "for idx, mol in enumerate(DG_mols):\n",
    "    if meta_substituted(mol) == True:\n",
    "        meta_sub_list.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ffab3bf-49a5-484f-8158-a5d5fb7d9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multi-task neural network model and dataset\n",
    "class MultiTaskNN(nn.Module):\n",
    "    def __init__(self, in_dim1,in_dim2,in_dim3, hidden_dim, regression_output_dim1, regression_output_dim2, num_classes):\n",
    "        super(MultiTaskNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_dim1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fc3 = nn.Linear(in_dim2, hidden_dim)\n",
    "        self.fc4 = nn.Linear(in_dim3, hidden_dim)\n",
    "        \n",
    "        self.regression_output1 = nn.Linear(hidden_dim, regression_output_dim1)\n",
    "        self.regression_output2 = nn.Linear(hidden_dim, regression_output_dim2)\n",
    "        self.classification_output = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, h1, h2, h3):\n",
    "            \n",
    "        h1 = F.relu(h1)\n",
    "        h1 = F.relu(self.fc1(h1))\n",
    "        h1 = F.relu(self.fc2(h1))\n",
    "        \n",
    "        h2 = F.relu(self.fc3(h2))\n",
    "        h2 = F.relu(self.fc2(h2))\n",
    "        \n",
    "        h3 = F.relu(self.fc4(h3))\n",
    "        h3 = F.relu(self.fc2(h3))\n",
    "        \n",
    "        regression_output1 = self.regression_output1(h1)\n",
    "        regression_output2 = self.regression_output2(h2)\n",
    "        classification_output = self.classification_output(h3)\n",
    "        \n",
    "        return regression_output1, regression_output2, classification_output\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, DG_list, RX_list, reaction_list, label1_list, label2_list, label3_list, num_list):\n",
    "        self.DG_list = DG_list\n",
    "        self.RX_list = RX_list\n",
    "        self.reaction_list = reaction_list\n",
    "        self.label1_list = label1_list\n",
    "        self.label2_list = label2_list\n",
    "        self.label3_list = label3_list\n",
    "        self.num_list = num_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.DG_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        DG = self.DG_list[index]\n",
    "        RX = self.RX_list[index]\n",
    "        reaction = self.reaction_list[index]\n",
    "        label1 = self.label1_list[index]\n",
    "        label2 = self.label2_list[index]\n",
    "        label3 = self.label3_list[index]\n",
    "        num = self.num_list[index]\n",
    "        return DG, RX, reaction, label1, label2, label3, num\n",
    "    \n",
    "target = torch.tensor(target, dtype=torch.long)\n",
    "num = torch.tensor(num, dtype=torch.long)\n",
    "target_DG = torch.tensor(target_DG, dtype=torch.float)\n",
    "target_RX = torch.tensor(target_RX, dtype=torch.float)\n",
    "DG_descs = torch.tensor(DG_descs, dtype=torch.float)\n",
    "RX_descs = torch.tensor(RX_descs, dtype=torch.float)\n",
    "all_descriptors = torch.tensor(all_descriptors, dtype=torch.float)\n",
    "dataset = MultiTaskDataset(DG_descs, RX_descs, all_descriptors, target_DG, target_RX, target, num)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=30,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf9fc353-ca53-4bb8-9d73-1c4f2a1fc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate(model, dataloader, o_sub_list, m_sub_list):\n",
    "    model.eval()\n",
    "    correct_task3 = 0\n",
    "    total = 0\n",
    "    num_list = []\n",
    "    pred_task1 = []\n",
    "    label_task1 = []\n",
    "    pred_task2 = []\n",
    "    label_task2 = []\n",
    "\n",
    "    for h1, h2, h3, labels1, labels2, labels3, num in dataloader:\n",
    "        \n",
    "        task1_output, task2_output, task3_output = model(h1, h2, h3)\n",
    "        \n",
    "        total += len(labels3)\n",
    "        _, predicted = torch.max(task3_output, dim=1)\n",
    "        \n",
    "        for i in range(len(labels3)):\n",
    "            if num[i] in o_sub_list:\n",
    "                if labels3[i] == predicted[i]:\n",
    "                    correct_task3 += 1\n",
    "                if labels3[i] == 4 and predicted[i] == 0:\n",
    "                    correct_task3 += 1\n",
    "                    predicted[i] = 4\n",
    "            elif num[i] in m_sub_list:\n",
    "                if labels3[i] == predicted[i]:\n",
    "                    correct_task3 += 1\n",
    "                if labels3[i] == 4 and predicted[i] == 0:\n",
    "                    correct_task3 += 1\n",
    "                    predicted[i] = 4\n",
    "                if labels3[i] == 3 and predicted[i] == 1:\n",
    "                    correct_task3 += 1\n",
    "                    predicted[i] = 3\n",
    "            else:\n",
    "                if labels3[i] == 0 and predicted[i] == 0:\n",
    "                    correct_task3 += 1\n",
    "                elif labels3[i] == 0 and predicted[i] == 4:\n",
    "                    correct_task3 += 1\n",
    "                elif labels3[i] == 1 and predicted[i] == 1:\n",
    "                    correct_task3 += 1\n",
    "                elif labels3[i] == 1 and predicted[i] == 3:\n",
    "                    correct_task3 += 1\n",
    "                elif labels3[i] == 2 and predicted[i] == 2:\n",
    "                    correct_task3 += 1\n",
    "\n",
    "        num_list.append(num)\n",
    "        task1_output = task1_output.tolist()\n",
    "        task2_output = task2_output.tolist()\n",
    "        labels1 = labels1.tolist()\n",
    "        labels2 = labels2.tolist()\n",
    "        pred_task1.extend(task1_output)\n",
    "        label_task1.extend(labels1)\n",
    "        pred_task2.extend(task2_output)\n",
    "        label_task2.extend(labels2)               \n",
    "        \n",
    "    accuracy_task3 = 1.0 * correct_task3 / total\n",
    "\n",
    "    MAE1 = mean_absolute_error(label_task1, pred_task1)\n",
    "    MAE2 = mean_absolute_error(label_task2, pred_task2)    \n",
    "    \n",
    "    label_task1 = np.array(label_task1)\n",
    "    pred_task1 = np.array(pred_task1)\n",
    "    label_task1_flat = label_task1.flatten()\n",
    "    pred_task1_flat = pred_task1.flatten()\n",
    "    r2_1, _ = pearsonr(label_task1_flat, pred_task1_flat)\n",
    "    \n",
    "    label_task2 = np.array(label_task2)\n",
    "    pred_task2 = np.array(pred_task2)\n",
    "    label_task2_flat = label_task2.flatten()\n",
    "    pred_task2_flat = pred_task2.flatten()\n",
    "    r2_2, _ = pearsonr(label_task2_flat, pred_task2_flat)\n",
    "\n",
    "    return accuracy_task3, predicted, labels3, num_list, MAE1, MAE2, r2_1, r2_2, label_task1, pred_task1, label_task2, pred_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1418e868-4eed-4216-a84b-5689ff5cf4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 | Loss 0.0626 | Train Acc. 0.9391 | Validation Acc. 0.7308 \n",
      "Fold 02 | Loss 0.0626 | Train Acc. 0.8913 | Validation Acc. 0.9615 \n",
      "Fold 03 | Loss 0.0543 | Train Acc. 0.9174 | Validation Acc. 0.8077 \n",
      "Fold 04 | Loss 0.0543 | Train Acc. 0.8826 | Validation Acc. 0.9231 \n",
      "Fold 05 | Loss 0.0543 | Train Acc. 0.8913 | Validation Acc. 0.9615 \n",
      "Fold 06 | Loss 0.0493 | Train Acc. 0.9217 | Validation Acc. 0.9231 \n",
      "Fold 07 | Loss 0.0493 | Train Acc. 0.9307 | Validation Acc. 0.7600 \n",
      "Fold 08 | Loss 0.0493 | Train Acc. 0.8788 | Validation Acc. 0.9600 \n",
      "Fold 09 | Loss 0.0436 | Train Acc. 0.9394 | Validation Acc. 0.9600 \n",
      "Fold 10 | Loss 0.0436 | Train Acc. 0.9091 | Validation Acc. 0.8800 \n",
      "Accuracy_all 0.8868, Loss 0.0523\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "kf = KFold(n_splits=10, shuffle=True,random_state=0)\n",
    "fold_num = 1\n",
    "all_acc_list = []\n",
    "all_loss_list = []\n",
    "r2_1_all_list = []\n",
    "r2_2_all_list = []\n",
    "MAE1_all_list = []\n",
    "MAE2_all_list = []\n",
    "target_list_cm = []\n",
    "pred_list_cm = []\n",
    "output1_list = []\n",
    "loss_list = []\n",
    "loss_list1 = []\n",
    "loss_list2 = []\n",
    "MAE1_val_list = []\n",
    "MAE2_val_list = []\n",
    "r2_1_val_list = []\n",
    "r2_2_val_list = []\n",
    "num_val_end = []\n",
    "pred1_all = []\n",
    "label1_all = []\n",
    "pred2_all = []\n",
    "label2_all = []\n",
    "\n",
    "for train_indices, val_indices in kf.split(dataset):\n",
    "    train_acc_list = []\n",
    "    valid_acc_list = [0.0]    \n",
    "    train_dataset = [dataset[i] for i in train_indices]\n",
    "    val_dataset = [dataset[i] for i in val_indices]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=30,\n",
    "        drop_last=False,\n",
    "        shuffle=True)\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=30,\n",
    "        drop_last=False,\n",
    "        shuffle=True)\n",
    "    \n",
    "    model = MultiTaskNN(DG_dim,RX_dim,reaction_dim, 250, 3, 3, 5)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    predicted_val_list = []\n",
    "    labels_val_list = []\n",
    "    num_val_list = []\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        for inputs_task1, inputs_task2, inputs_task3, labels_task1, labels_task2, labels_task3, num in train_loader:\n",
    "            opt.zero_grad()\n",
    "            output1, output2, output3 = model(inputs_task1, inputs_task2, inputs_task3)\n",
    "            output1_list.extend(output1)\n",
    "            \n",
    "            loss1 = F.mse_loss(output1, labels_task1)\n",
    "            loss2 = F.l1_loss(output2, labels_task2)\n",
    "            loss3 = F.cross_entropy(output3, labels_task3)\n",
    "            total_loss = loss1*0.4 + loss2*0.4 + loss3*0.2\n",
    "            \n",
    "            total_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        train_acc, predicted_train, labels_train, num_train, MAE1_t, MAE2_t, r2_1_t, r2_2_t, _, _, _, _ = evaluate(model, train_loader, ortho_sub_list, meta_sub_list)\n",
    "        train_acc_list.append(train_acc)\n",
    "        valid_acc, predicted_val, labels_val, num_val, MAE1_val, MAE2_val, r2_1_val, r2_2_val, label1_v, pred1_v, label2_v, pred2_v = evaluate(model, val_loader, ortho_sub_list, meta_sub_list)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        MAE1_val_list.append(MAE1_val)\n",
    "        MAE2_val_list.append(MAE2_val)\n",
    "        r2_1_val_list.append(r2_1_val)\n",
    "        r2_2_val_list.append(r2_2_val)\n",
    "        loss4 = total_loss\n",
    "        loss4 = loss4.detach() \n",
    "        loss1 = loss1.detach() \n",
    "        loss2 = loss2.detach() \n",
    "        loss4 = loss4.numpy()\n",
    "        loss1 = loss1.numpy()\n",
    "        loss2 = loss2.numpy()\n",
    "        loss_list.append(loss4)\n",
    "        loss_list1.append(loss1)\n",
    "        loss_list2.append(loss2)\n",
    "        #print(\"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(epoch, total_loss , train_acc, valid_acc))\n",
    "            \n",
    "        if max(valid_acc_list[:-1]) < valid_acc_list[-1]:\n",
    "            num_val_end_m = torch.empty(0)\n",
    "            target_max = []\n",
    "            pred_max = []\n",
    "            pred1 = []\n",
    "            label1 = []\n",
    "            pred2 = []\n",
    "            label2 = []\n",
    "            \n",
    "            num_val_end_m = torch.cat((num_val_end_m, num_val[0]), dim=0)\n",
    "            target_max.append(labels_val)\n",
    "            pred_max.append(predicted_val)\n",
    "            label1.append(label1_v)\n",
    "            pred1.append(pred1_v)\n",
    "            label2.append(label2_v)\n",
    "            pred2.append(pred2_v)\n",
    "        \n",
    "    num_val_end.append(num_val_end_m)\n",
    "    pred1_all.append(pred1)\n",
    "    label1_all.append(label1)\n",
    "    pred2_all.append(pred2)\n",
    "    label2_all.append(label2)\n",
    "    \n",
    "    target_list_cm.extend(target_max)\n",
    "    pred_list_cm.extend(pred_max)\n",
    "    \n",
    "    print(\n",
    "        \"Fold {:02d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n",
    "        fold_num, min(loss_list) , max(train_acc_list), max(valid_acc_list))\n",
    "        )\n",
    "    all_acc_list.append(max(valid_acc_list))\n",
    "    all_loss_list.append(min(loss_list))\n",
    "    r2_1_all_list.append(max(r2_1_val_list))\n",
    "    r2_2_all_list.append(max(r2_2_val_list))\n",
    "    MAE1_all_list.append(MAE1_val_list[-1])\n",
    "    MAE2_all_list.append(MAE2_val_list[-1])\n",
    "    fold_num += 1\n",
    "\n",
    "average_accuracy = np.mean(all_acc_list)\n",
    "average_loss = np.mean(all_loss_list)\n",
    "average_r2_1 = np.mean(r2_1_all_list)\n",
    "average_r2_2 = np.mean(r2_2_all_list)\n",
    "average_MAE1 = np.mean(MAE1_all_list)\n",
    "average_MAE2 = np.mean(MAE2_all_list)\n",
    "print(\"Accuracy_all {:.4f}, Loss {:.4f}\".format(average_accuracy, average_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
